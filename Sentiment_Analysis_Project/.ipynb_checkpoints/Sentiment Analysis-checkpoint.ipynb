{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import matplotlib .pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment:\n",
    "    NEGATIVE = \"NEGATIVE\"\n",
    "    NEUTRAL = \"NEUTRAL\"\n",
    "    POSITIVE = \"POSITIVE\"\n",
    "    \n",
    "    \n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment()\n",
    "\n",
    "    def get_sentiment(self):\n",
    "        if self.score <= 2:\n",
    "            return Sentiment.NEGATIVE\n",
    "        elif self.score == 3:\n",
    "            return Sentiment.NEUTRAL\n",
    "        else:  # Score of 4 or 5\n",
    "            return Sentiment.POSITIVE\n",
    "        \n",
    "        \n",
    "class ReviewContainer:\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "\n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.reviews]\n",
    "\n",
    "    def get_sentiment(self):\n",
    "        return [x.sentiment for x in self.reviews]\n",
    "\n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.sentiment == Sentiment.NEGATIVE, self.reviews))\n",
    "        positive = list(filter(lambda x: x.sentiment == Sentiment.POSITIVE, self.reviews))\n",
    "        positive_shrunk = positive[:len(negative)]\n",
    "        self.reviews = negative + positive_shrunk\n",
    "        random.shuffle(self.reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name = 'books_small_10000.json'\n",
    "\n",
    "reviews = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        reviews.append(Review(review['reviewText'], review['overall']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training, test = train_test_split(reviews, test_size=0.33, random_state=42)\n",
    "\n",
    "train_container = ReviewContainer(training)\n",
    "\n",
    "test_container = ReviewContainer(test)\n",
    "\n",
    "train_container.evenly_distribute()\n",
    "train_x_bow = train_container.get_text()\n",
    "train_y_bow = train_container.get_sentiment()\n",
    "\n",
    "test_container.evenly_distribute()\n",
    "test_x_bow = test_container.get_text()\n",
    "test_y_bow = test_container.get_sentiment()\n",
    "\n",
    "#print(train_y_bow.count(Sentiment.POSITIVE))\n",
    "#print(train_y_bow.count(Sentiment.NEGATIVE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data for TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training2, test2 = train_test_split(reviews, test_size=0.33, random_state=17)\n",
    "\n",
    "train_container_2 = ReviewContainer(training2)\n",
    "\n",
    "test_container_2 = ReviewContainer(test2)\n",
    "\n",
    "train_container_2.evenly_distribute()\n",
    "train_x_tfidf = train_container_2.get_text()\n",
    "train_y_tfidf = train_container_2.get_sentiment()\n",
    "\n",
    "test_container_2.evenly_distribute()\n",
    "test_x_tfidf = test_container_2.get_text()\n",
    "test_y_tfidf = test_container_2.get_sentiment()\n",
    "\n",
    "#print(train_y_tfidf.count(Sentiment.POSITIVE))\n",
    "#print(train_y_tfidf.count(Sentiment.NEGATIVE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "train_x_vectors_bow = bow_vectorizer.fit_transform(train_x_bow)\n",
    "\n",
    "test_x_vectors_bow = bow_vectorizer.transform(test_x_bow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()\n",
    "\n",
    "train_x_vectors_tfidf = tfidf.fit_transform(train_x_tfidf)\n",
    "\n",
    "test_x_vectors_tfidf = tfidf.transform(test_x_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Log_Reg = LogisticRegression(random_state=0,solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag of Words Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8149038461538461\n"
     ]
    }
   ],
   "source": [
    "Log_Reg.fit(train_x_vectors_bow,train_y_bow)\n",
    "print(Log_Reg.score(test_x_vectors_bow, test_y_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score BOW LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82051282, 0.808933  ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lr_bow_f1=f1_score(test_y_bow, Log_Reg.predict(test_x_vectors_bow), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE])\n",
    "lr_bow_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8469387755102041\n"
     ]
    }
   ],
   "source": [
    "final_model=Log_Reg.fit(train_x_vectors_tfidf,train_y_tfidf)\n",
    "print(Log_Reg.score(test_x_vectors_tfidf, test_y_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score TFIDF LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84210526, 0.85148515])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lr_tfidf_f1=f1_score(test_y_tfidf, Log_Reg.predict(test_x_vectors_tfidf), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE])\n",
    "lr_tfidf_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf_svm = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980769230769231\n"
     ]
    }
   ],
   "source": [
    "clf_svm.fit(train_x_vectors_bow, train_y_bow)\n",
    "print(clf_svm.score(test_x_vectors_bow, test_y_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score BOW SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8028169 , 0.79310345])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "svm_bow_f1=f1_score(test_y_bow, clf_svm.predict(test_x_vectors_bow), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE])\n",
    "svm_bow_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8443877551020408\n"
     ]
    }
   ],
   "source": [
    "clf_svm.fit(train_x_vectors_tfidf, train_y_tfidf)\n",
    "print(clf_svm.score(test_x_vectors_tfidf, test_y_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score TFIDF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83905013, 0.84938272])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "svm_tfidf_f1=f1_score(test_y_tfidf, clf_svm.predict(test_x_vectors_tfidf), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE])\n",
    "svm_tfidf_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dec = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6322115384615384\n"
     ]
    }
   ],
   "source": [
    "clf_dec.fit(train_x_vectors_bow, train_y_bow)\n",
    "print(clf_dec.score(test_x_vectors_bow, test_y_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score BOW DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6313253 , 0.63309353])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "dt_bow_f1=f1_score(test_y_bow,clf_dec.predict(test_x_vectors_bow), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE])\n",
    "dt_bow_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6811224489795918\n"
     ]
    }
   ],
   "source": [
    "clf_dec.fit(train_x_vectors_tfidf, train_y_tfidf)\n",
    "print(clf_dec.score(test_x_vectors_tfidf, test_y_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score TFIDF DT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68193384, 0.68030691])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "dt_tfidf_f1=f1_score(test_y_tfidf, clf_dec.predict(test_x_vectors_tfidf), average=None, labels=[Sentiment.POSITIVE, Sentiment.NEGATIVE])\n",
    "dt_tfidf_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW comparision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>LogisticRegression(Bag-of-Words)</td>\n",
       "      <td>LinearSVM(Bag-of-Words)</td>\n",
       "      <td>DecisionTree(Bag-of-Words)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_Score</th>\n",
       "      <td>[0.8205128205128205, 0.8089330024813896]</td>\n",
       "      <td>[0.8028169014084507, 0.7931034482758621]</td>\n",
       "      <td>[0.6313253012048193, 0.6330935251798561]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 1  \\\n",
       "Model             LogisticRegression(Bag-of-Words)   \n",
       "F1_Score  [0.8205128205128205, 0.8089330024813896]   \n",
       "\n",
       "                                                 2  \\\n",
       "Model                      LinearSVM(Bag-of-Words)   \n",
       "F1_Score  [0.8028169014084507, 0.7931034482758621]   \n",
       "\n",
       "                                                 3  \n",
       "Model                   DecisionTree(Bag-of-Words)  \n",
       "F1_Score  [0.6313253012048193, 0.6330935251798561]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Algo_1 = ['LogisticRegression(Bag-of-Words)','LinearSVM(Bag-of-Words)','DecisionTree(Bag-of-Words)']\n",
    "\n",
    "f1_score_1 = [lr_bow_f1,svm_bow_f1,dt_bow_f1]\n",
    "\n",
    "comparision_1 = pd.DataFrame({'Model':Algo_1,'F1_Score':f1_score_1},index=[i for i in range(1,4)])\n",
    "\n",
    "comparision_1.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>LogisticRegression(TF-IDF)</td>\n",
       "      <td>LinearSVM(TF-IDF)</td>\n",
       "      <td>DecisionTree(TF-IDF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_Score</th>\n",
       "      <td>[0.8421052631578948, 0.8514851485148514]</td>\n",
       "      <td>[0.8390501319261213, 0.8493827160493828]</td>\n",
       "      <td>[0.6819338422391857, 0.680306905370844]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 1  \\\n",
       "Model                   LogisticRegression(TF-IDF)   \n",
       "F1_Score  [0.8421052631578948, 0.8514851485148514]   \n",
       "\n",
       "                                                 2  \\\n",
       "Model                            LinearSVM(TF-IDF)   \n",
       "F1_Score  [0.8390501319261213, 0.8493827160493828]   \n",
       "\n",
       "                                                3  \n",
       "Model                        DecisionTree(TF-IDF)  \n",
       "F1_Score  [0.6819338422391857, 0.680306905370844]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Algo_2 = ['LogisticRegression(TF-IDF)','LinearSVM(TF-IDF)','DecisionTree(TF-IDF)']\n",
    "\n",
    "f1_score_2 = [lr_tfidf_f1,svm_tfidf_f1,dt_tfidf_f1]\n",
    "\n",
    "comparision_2 = pd.DataFrame({'Model':Algo_2,'F1_Score':f1_score_2},index=[i for i in range(1,4)])\n",
    "\n",
    "comparision_2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as logisticRegression(tfidf) has best f1 score it will be the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./models/sentiment_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/sentiment_classifier.pkl', 'rb') as f:\n",
    "    loaded_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plot outline itself is interesting, but the 'hero' is a unconvincing Mary Sue. The challenges take too long to read, but not long enough book time to be reasonable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_x_tfidf[0])\n",
    "loaded_clf.predict(test_x_vectors_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
